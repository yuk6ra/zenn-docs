---
title: "【GR00T N1 / LeRobot】SO-ARM101の学習データでファインチューニングと推論する方法"
emoji: "🗂"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: []
published: false
---

# 概要

デモ。

NVIDIA GR00T N1でファインチューニングを行う。その後推論を行う。

参考文献
- https://github.com/NVIDIA/Isaac-GR00T/tree/d5984002e24d418872adc5822a5bbb1d6a9b4ddc
- https://github.com/huggingface/lerobot/tree/519b76110efeea55a4f919895d0029dc0df41e8b
- https://huggingface.co/blog/nvidia/gr00t-n1-5-so101-tuning
- https://github.com/NVIDIA/Isaac-GR00T

データを

# データ収集編

Penは複雑でうまく行かなかったので、より簡単なエピソードを個人的には推奨する。

LeRobotで50エピソードを目安に行なう。今回は一番簡単なデータを使う。

## タスクの種類
### **①複雑なタスク**: `so101-pen-cleanup`
- 

https://huggingface.co/spaces/lerobot/visualize_dataset?path=%2Fyuk6ra%2Fso101-pen-cleanup%2Fepisode_0

### 簡単なタスク:  `so101-tapes-cleanup`
- タイプ: 簡単なタスク

https://huggingface.co/spaces/lerobot/visualize_dataset?path=%2Fyuk6ra%2Fso101-tapes-cleanup%2Fepisode_0

### 最も簡単なタスク:`so101-onetape-cleanup`

https://huggingface.co/spaces/lerobot/visualize_dataset?path=%2Fyuk6ra%2Fso101-onetape-cleanup%2Fepisode_0


なお、下記のような設定で取得している。
カメラ設定では、ロボット視点での
- `tip`: 先端のカメラ
- `front`: 前方のカメラ
を用意した。カメラの名称が後々関わってくるので注意する。`wrist`と`front`であればそのままで使える。

```yaml
dataset:
  repo_id: "yuk6ra/so101-onetape-cleanup"  # Hugging Face のリポジトリID
  single_task: "Grab the tape and place it in the box."  # タスクの説明
  num_episodes: 50  # 記録するエピソード数
  fps: 30  # フレームレート
  episode_time_s: 15  # 1エピソードあたりの最大時間（秒）
  reset_time_s: 15  # エピソード記録後のリセット時間（秒）

# フォロワーアーム
robot:
  type: "so101_follower"
  port: "/dev/ttyACM0"  # シリアルポート
  id: "white"  # フォロワーのID
  
  # カメラ設定
  cameras:
    tip:
      type: "opencv"
      index_or_path: 0
      fps: 30
      width: 640
      height: 480
    front:
      type: "opencv"
      index_or_path: 2
      fps: 30
      width: 640
      height: 480

# リーダーアーム
teleop:
  type: "so101_leader" 
  port: "/dev/ttyACM1"  # シリアルポート
  id: "black"  # リーダーのID

# 追加オプション
options:
  display_data: false  # カメラ映像を表示するか
  push_to_hub: true # Hugging Face Hub に自動アップロードするか
```

# ファインチューニング編

## GPUを借りる
もし自分のものがあれば良い。

スペックは、下記。
- GPU: H100 SXM
    - VRAM: 80GB
- DISK: 300GB
- RAM: 128GB以上
- OS：Ubuntu 24.04
- ネット速度
    - Internet Upload Speed: 4Gbps
    - Internet Download Speed: 4Gbps

DISKは200GBぐらいはほしい。Checkpointにもよる。
今回は5000ステップで、約100GBを消費した。

## PC環境の確認

リモートのsshで接続する

```sh
ssh -p 30454 root@xxx.xxx.xxx.xx -L 8080:localhost:8080
```

### RAMの確認
```sh
$ free -h
               total        used        free      shared  buff/cache   available
Mem:           503Gi        34Gi       372Gi        47Mi       101Gi       469Gi
Swap:          8.0Gi       186Mi       7.8Gi
```

### GPUの確認
```sh
$ nvidia-smi
Sun Jul 13 06:57:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   47C    P0             73W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
```

### OSの確認
```sh
$ lsb_release -d
No LSB modules are available.
Description:    Ubuntu 24.04.2 LTS
```

### ディスクの確認
```sh
$ df /home -h
Filesystem      Size  Used Avail Use% Mounted on
overlay         300G   90M  300G   1% /
```

## 仮想環境の構築

https://huggingface.co/blog/nvidia/gr00t-n1-5-so101-tuning

公式に習う。

```sh
# クローンする
git clone https://github.com/NVIDIA/Isaac-GR00T
cd Isaac-GR00T

# 環境をつくる
conda create -n gr00t python=3.10
conda activate gr00t
pip install --upgrade setuptools
pip install -e .[base]
pip install --no-build-isolation flash-attn==2.7.1.post4
```


## ログイン
各種サービスに先にログインしておく。

### Hugging Face
Setting Pageより
https://huggingface.co/settings/tokens

```sh
huggingface-cli login
```

### Wandb
アクセスキーを取る
https://wandb.ai/authorize


```sh
wandb login
```



## 学習データのダウンロード

自分に合わせて。Huggingfaceからデータをもらってくる。

```sh
huggingface-cli download \
       --repo-type dataset yuk6ra/so101-onetape-cleanup \
       --local-dir ./demo_data/so101-onetape-cleanup
```

GR00T互換にするために`modality.json`がデータセットの`/meta/`以下に必要です。今回はデュアルカメラを用いています。

```sh
cp getting_started/examples/so100_dualcam__modality.json ./demo_data/so101-onetape-cleanup/meta/modality.json
```

そして、`modality.json`が`wrist`から
```sh
vim ./demo_data/so101-onetape-cleanup/meta/modality.json
```

```json
 {
    ...
        "video": {
            "front": {
                "original_key": "observation.images.front"
            },
            "tip": {　// `wrist`から`tip`へ変更
                "original_key": "observation.images.tip"  // `wrist`から`tip`へ変更
            }
        },
    ...
```

```shell
python scripts/load_dataset.py \
    --dataset-path ./demo_data/so101-onetape-cleanup \
    --plot-state-action \
    --video-backend torchvision_av
```

```sh
====================================================================================================
========================================= Humanoid Dataset =========================================
====================================================================================================
{'action.gripper': 'np scalar: 1.1111111640930176 [1, 1] float64',
 'action.single_arm': 'np: [1, 5] float64',
 'annotation.human.task_description': ['Grab the tape and place it in the '
                                       'box.'],
 'state.gripper': 'np scalar: 2.410423517227173 [1, 1] float64',
 'state.single_arm': 'np: [1, 5] float64',
 'video.front': 'np: [1, 480, 640, 3] uint8',
 'video.tip': 'np: [1, 480, 640, 3] uint8'}
dict_keys(['video.front', 'video.tip', 'state.single_arm', 'state.gripper', 'action.single_arm', 'action.gripper', 'annotation.human.task_description'])
==================================================
video.front: (1, 480, 640, 3)
video.tip: (1, 480, 640, 3)
state.single_arm: (1, 5)
state.gripper: (1, 1)
action.single_arm: (1, 5)
action.gripper: (1, 1)
annotation.human.task_description: ['Grab the tape and place it in the box.']
Image 0, prompt: ['Grab the tape and place it in the box.']
Image 10, prompt: ['Grab the tape and place it in the box.']
Image 20, prompt: ['Grab the tape and place it in the box.']
Image 30, prompt: ['Grab the tape and place it in the box.']
Image 40, prompt: ['Grab the tape and place it in the box.']
Image 50, prompt: ['Grab the tape and place it in the box.']
Image 60, prompt: ['Grab the tape and place it in the box.']
Image 70, prompt: ['Grab the tape and place it in the box.']
Image 80, prompt: ['Grab the tape and place it in the box.']
Image 90, prompt: ['Grab the tape and place it in the box.']
Image 100, prompt: ['Grab the tape and place it in the box.']
Image 110, prompt: ['Grab the tape and place it in the box.']
Image 120, prompt: ['Grab the tape and place it in the box.']
Image 130, prompt: ['Grab the tape and place it in the box.']
Image 140, prompt: ['Grab the tape and place it in the box.']
Image 150, prompt: ['Grab the tape and place it in the box.']
Image 160, prompt: ['Grab the tape and place it in the box.']
Image 170, prompt: ['Grab the tape and place it in the box.']
Image 180, prompt: ['Grab the tape and place it in the box.']
Image 190, prompt: ['Grab the tape and place it in the box.']
Warning: Skipping left_arm as it's not found in both state and action dictionaries
Warning: Skipping right_arm as it's not found in both state and action dictionaries
Warning: Skipping left_hand as it's not found in both state and action dictionaries
Warning: Skipping right_hand as it's not found in both state and action dictionaries
Plotted state and action space
```

## 学習させる

H100で30分ほど放置。
MAX5000ステップで100GBほどを消費。

```sh
python scripts/gr00t_finetune.py \
      --dataset-path ./demo_data/so101-onetape-cleanup/ \
      --num-gpus 1 \
      --output-dir ./so101-checkpoints  \
      --max-steps 5000 \
      --data-config so100_dualcam \
      --video-backend torchvision_av
```

### エラー：`ValueError: Video key wrist not found in dataset metadata. Available keys: dict_keys(['front', 'tip'])`

もしカメラ名称が違う。

```sh
vim ./gr00t/experiment/data_config.py
```

225行目あたり。
```python
class So100DualCamDataConfig(So100DataConfig):
 -   video_keys = ["video.front", "video.wrist"]
 +  video_keys = ["video.front", "video.tip"]
```

### エラー①: `av.error.MemoryError: [Errno 12] Cannot allocate memory`
もし下記のようなエラーがでるなら。
```sh
  0%|                                                                                                                            | 0/5000 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/workspace/Isaac-GR00T/scripts/gr00t_finetune.py", line 315, in <module>
    main(config)
  File "/workspace/Isaac-GR00T/scripts/gr00t_finetune.py", line 287, in main
    experiment.train()
  File "/workspace/Isaac-GR00T/gr00t/experiment/runner.py", line 173, in train
    self.trainer.train(resume_from_checkpoint=self.resume_from_checkpoint)
  File "/workspace/Isaac-GR00T/gr00t/experiment/trainer.py", line 153, in train
    return super().train(resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
  File "/venv/gr00t/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/venv/gr00t/lib/python3.10/site-packages/transformers/trainer.py", line 2514, in _inner_training_loop
    batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches, args.device)
  File "/venv/gr00t/lib/python3.10/site-packages/transformers/trainer.py", line 5243, in get_batch_samples
    batch_samples.append(next(epoch_iterator))
  File "/venv/gr00t/lib/python3.10/site-packages/accelerate/data_loader.py", line 552, in __iter__
    current_batch = next(dataloader_iter)
  File "/venv/gr00t/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
    data = self._next_data()
  File "/venv/gr00t/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1465, in _next_data
    return self._process_data(data)
  File "/venv/gr00t/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1491, in _process_data
    data.reraise()
  File "/venv/gr00t/lib/python3.10/site-packages/torch/_utils.py", line 714, in reraise
    raise RuntimeError(msg) from None
RuntimeError: Caught MemoryError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/venv/gr00t/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 351, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/venv/gr00t/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/venv/gr00t/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/workspace/Isaac-GR00T/gr00t/data/dataset.py", line 508, in __getitem__
    return self.transforms(self.get_step_data(trajectory_id, base_index))
  File "/workspace/Isaac-GR00T/gr00t/data/dataset.py", line 542, in get_step_data
    data[key] = self.get_data_by_modality(trajectory_id, modality, key, base_index)
  File "/workspace/Isaac-GR00T/gr00t/data/dataset.py", line 802, in get_data_by_modality
    return self.get_video(trajectory_id, key, base_index)
  File "/workspace/Isaac-GR00T/gr00t/data/dataset.py", line 672, in get_video
    return get_frames_by_timestamps(
  File "/workspace/Isaac-GR00T/gr00t/utils/video.py", line 112, in get_frames_by_timestamps
    for frame in reader:
  File "/venv/gr00t/lib/python3.10/site-packages/torchvision/io/video_reader.py", line 200, in __next__
    frame = next(self._c)
  File "av/container/input.pyx", line 208, in decode
  File "av/packet.pyx", line 80, in av.packet.Packet.decode
  File "av/video/stream.pyx", line 41, in av.video.stream.VideoStream.decode
  File "av/video/stream.pyx", line 50, in av.video.stream.VideoStream.decode
  File "av/codec/context.pyx", line 462, in av.codec.context.CodecContext.decode
  File "av/codec/context.pyx", line 238, in av.codec.context.CodecContext.open
  File "av/error.pyx", line 326, in av.error.err_check
av.error.MemoryError: [Errno 12] Cannot allocate memory
```

いったんPyAVをアップデートする
```sh
pip install -U av
```

## モデルのアップロード
```sh
# モデルのアップロード
cd so101-checkpoints/checkpoint-5000/

# 削除
# rm -rf scheduler.pt 
# rm -rf optimizer.pt

# アップロードする
huggingface-cli upload \ 
      --repo-type model yuk6ra/so101-onetape-cleanup . \
      --commit-message="5000 step"
```

アップロードし終えたら、借りていたサーバーはすぐにインスタンスを削除しよう。忘れずに注意。

# 推論編

推論サーバーとクライアントノードに別れます。

## 推論サーバーの仮想環境の構築

ローカルの場合とクラウドの場合に分かれる。

### ローカルの場合
ローカルでも同じように、環境を作る。
推論サーバーが別PCであれば、もう一度GR00T N1の環境をつくる。

同様の環境を作る。

```sh
git clone https://github.com/NVIDIA/Isaac-GR00T
cd Isaac-GR00T
conda create -n gr00t python=3.10
conda activate gr00t
pip install --upgrade setuptools
pip install -e .[base]
pip install --no-build-isolation flash-attn==2.7.1.post4
```

参考文献：https://huggingface.co/docs/lerobot/installation

### クラウドの場合

イメージなどに下記の任意のポートをあけてください。
デフォルトでは5000になっています。
```
-p 5555:5555
```

## モデルのダウンロード

```sh
# ログイン
huggingface-cli login


# ダウンロード
huggingface-cli download \
     --repo-type model yuk6ra/so101-onetape-cleanup \
     --local-dir ./model/so101-onetape-cleanup
```

## 推論サーバーの立ち上げ
### ローカル環境
- 4070 Ti 12GB
- 128GB
- Ubuntu 22.04
- 

```sh
$ nvidia-smi
Sun Jul 13 19:45:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.153.02             Driver Version: 570.153.02     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4070 Ti     Off |   00000000:01:00.0  On |                  N/A |
|  0%   34C    P8              6W /  285W |     682MiB /  12282MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            2453      G   /usr/lib/xorg/Xorg                      309MiB |
|    0   N/A  N/A            2608      G   /usr/bin/gnome-shell                     70MiB |
|    0   N/A  N/A            3211      G   /opt/brave.com/brave/brave                3MiB |
|    0   N/A  N/A            3254      G   ...c0b8c13d56e6779503f2cc987a424        119MiB |
|    0   N/A  N/A            5526      G   ...ess --variations-seed-version        125MiB |
+-----------------------------------------------------------------------------------------+
```


```sh
$ lsb_release -d
Description:	Ubuntu 22.04.5 LTS
```

立ち上げる
```sh
python scripts/inference_service.py --model_path ./model/so101-onetape-cleanup --embodiment_tag new_embodiment --data_config so100_dualcam --server --port 5555
```

下記の状態になれば立ち上がった。
```
Server is ready and listening on tcp://0.0.0.0:5555
```

なお、`so100_dualcam`でまた同じく、なおす。`gr00t/experiment/data_config.py`を直す。直さないと`ValueError: Video key wrist not found in dataset metadata. Available keys: dict_keys(['front', 'tip'])`とでる。

####　エラー
```sh
pip install --no-build-isolation flash-attn==2.7.1.post4 

Collecting flash-attn==2.7.1.post4
  Using cached flash_attn-2.7.1.post4.tar.gz (2.7 MB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [20 lines of output]
      fatal: not a git repository (or any of the parent directories): .git
      /tmp/pip-install-myxzi7su/flash-attn_3ea6f071d2e84485a9e98af6137eb7b7/setup.py:99: UserWarning: flash_attn was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.
        warnings.warn(
      Traceback (most recent call last):
        File "<string>", line 2, in <module>
        File "<pip-setuptools-caller>", line 35, in <module>
        File "/tmp/pip-install-myxzi7su/flash-attn_3ea6f071d2e84485a9e98af6137eb7b7/setup.py", line 184, in <module>
          CUDAExtension(
        File "/home/yuk6ra/anaconda3/envs/gr00t/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 1078, in CUDAExtension
          library_dirs += library_paths(cuda=True)
        File "/home/yuk6ra/anaconda3/envs/gr00t/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 1209, in library_paths
          if (not os.path.exists(_join_cuda_home(lib_dir)) and
        File "/home/yuk6ra/anaconda3/envs/gr00t/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 2416, in _join_cuda_home
          raise OSError('CUDA_HOME environment variable is not set. '
      OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.
      
      
      torch.__version__  = 2.5.1+cu124
      
      
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```

これを入れる
```
conda install -c nvidia cuda-toolkit=12.4
```

#### エラー
もし`ModuleNotFoundError: No module named 'flash_attn'`が出たら、仮想環境が`gr00t`ではなく`lerobot`や`base`の環境になっていないかを確認する。

## クライアントサーバーの立ち上げ

学習データ収集のときにLerobotの環境を作っていると思います。

LeRobotのライブラリと互換性を保つ必要があります。lerobotの環境をつくるやる。もうすでにあればOKで学習データを収集したときのものでOK。

念の為。
```sh
git clone https://github.com/huggingface/lerobot.git
cd lerobot
conda create -y -n lerobot python=3.10
conda activate lerobot
conda install ffmpeg -c conda-forge
pip install -e .
pip install -e ".[feetech]"
```

Isaac-GR00Tのフォルダに移動して環境を切り替える。


`Isaac-GR00T`まで移動して、
```sh
# 移動
cd ~/Documents/Isaac-GR00T/

# lerobotを立ち上げる
conda activate lerobot
```

さらに追加でパッケージとgr00tのパッケージのダウンロード。
```sh
pip install matplotlib
```


`outputs/captured_images`以下でカメラのIDをメモしておく。
`tip`が`2`、`front`が`0`であった。
```sh
python -m lerobot.find_cameras opencv
```

`getting_started/examples/eval_lerobot.py`で、lerobotのバージョンがあっていないので`common`を消す。

```python
from lerobot.cameras.opencv.configuration_opencv import ( # <---commonを消す
    OpenCVCameraConfig,
)
from lerobot.robots import ( # <---commonを消す
    Robot,
    RobotConfig,
    koch_follower,
    make_robot_from_config,
    so100_follower,
    so101_follower,
)
from lerobot.utils.utils import ( # <---commonを消す
    init_logging,
    log_say,
)

# NOTE:
# Sometimes we would like to abstract different env, or run this on a separate machine
# User can just move this single python class method gr00t/eval/service.py
# to their code or do the following line below
import os # <--- 追加
import sys # <--- 追加
sys.path.append(os.path.expanduser("./gr00t/eval/")) # <--- パスの修正
from service import ExternalRobotInferenceClient
```

```sh
python getting_started/examples/eval_lerobot.py \
         --robot.type=so101_follower \
         --robot.port=/dev/ttyACM1 \
         --robot.id=white \
         --robot.cameras="{
             tip: {type: opencv, index_or_path: 2, width: 640, height: 480, fps: 30},
             front: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}
         }" \
         --lang_instruction="Grab the tape and place it in the box."
```

### クラウド向け
ローカル環境から推論サーバーに向けて、
```sh
python getting_started/examples/eval_lerobot.py \
       --robot.type=so101_follower \
       --robot.port=/dev/ttyACM0 \
       --robot.id=white \
       --robot.cameras="{
           tip: {type: opencv, index_or_path: 2, width: 640, height: 480, fps: 30},
           front: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}
       }" \
       --policy_host 194.14.47.19 \ # 追加
       --policy_port 22037 \ # 追加
       --lang_instruction="Grab tapes and place into pen holder."
```


# 評価編
評価してみる。
## 評価する

データを落とす。
```shell
huggingface-cli download \
       --repo-type dataset yuk6ra/so101-onetape-cleanup \
       --local-dir ./demo_data/so101-onetape-cleanup
```

ここをよしなに修正する。
```shell
cp getting_started/examples/so100_dualcam__modality.json ./demo_data/so101-onetape-cleanup/meta/modality.json
```

適当に`./demo_data/so101-onetape-cleanup/meta/modality.json`を直す。

```json
 {
    ...
        "video": {
            "front": {
                "original_key": "observation.images.front"
            },
            "tip": {　// `wrist`から`tip`へ変更
                "original_key": "observation.images.tip"  // `wrist`から`tip`へ変更
            }
        },
    ...
```

ポリシーを評価する。

```shell
python scripts/eval_policy.py \
    --plot \
    --embodiment_tag new_embodiment \
    --model_path ./model/so101-onetape-cleanup/ \
    --data_config so100_dualcam \
    --dataset_path ./demo_data/so101-onetape-cleanup/ \
    --video_backend torchvision_av \
    --modality_keys single_arm gripper \
    --denoising_steps 4
```

結果、
![](/images/c474627fe7775b/plot.png)

なお、`so100_dualcam`を直すために、
`gr00t/experiment/data_config.py`の**225行目**には注意して書き換える。

```python
class So100DualCamDataConfig(So100DataConfig):
- video_keys = ["video.front", "video.wrist"]
+ video_keys = ["video.front", "video.tip"]
```

### エラー：`ModuleNotFoundError: No module named 'tyro'`

もし`ModuleNotFoundError: No module named 'flash_attn'`が出たら、仮想環境が`gr00t`ではなく`lerobot`や`base`の環境になっていないかを確認する。


## 学習データの準備

```sh
huggingface-cli download \
       --repo-type dataset yuk6ra/so101-onetape-cleanup \
       --local-dir ./demo_data/so101-onetape-cleanup

cp getting_started/examples/so100_dualcam__modality.json ./demo_data/so101-onetape-cleanup/meta/modality.json
```


必要であれば直す。
```sh
vim ./demo_data/so101-onetape-cleanup/meta/modality.json
```

## モデルの準備

ステップごとにあれば。
```sh
huggingface-cli download \
    --repo-type model  yuk6ra/so101-onetape-cleanup \
    --local-dir ./model/so101-onetape-cleanup
    # --revision step-5000
```

## 評価する

```sh
python scripts/eval_policy.py --plot \
       --embodiment_tag new_embodiment \
       --model_path ./model/so101-onetape-cleanup/ \
       --data_config so100_dualcam \
       --dataset_path ./demo_data/so101-onetape-cleanup/ \
       --video_backend torchvision_av \
       --modality_keys single_arm gripper \
       --denoising_steps 4
```

## 比較検討する

2000と5000


# まとめ

あああ
