---
title: "【GR00T N1 / LeRobot】SO-ARM101の学習データでファインチューニングと推論する方法"
emoji: "🗂"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: []
published: false
---

# 概要

NVIDIA GR00T N1でファインチューニングを行う。その後推論を行う。

参考文献
- https://github.com/NVIDIA/Isaac-GR00T/tree/d5984002e24d418872adc5822a5bbb1d6a9b4ddc
- https://github.com/huggingface/lerobot/tree/519b76110efeea55a4f919895d0029dc0df41e8b
- https://huggingface.co/blog/nvidia/gr00t-n1-5-so101-tuning
- https://github.com/NVIDIA/Isaac-GR00T

データを

# データ収集編

Penは複雑でうまく行かなかったので、より簡単なエピソードを個人的には推奨する。

LeRobotで50エピソードを目安に行なう。


複雑な学習データ
https://huggingface.co/spaces/lerobot/visualize_dataset?path=%2Fyuk6ra%2Fso101-pen-cleanup%2Fepisode_0

簡単な学習データ
https://huggingface.co/spaces/lerobot/visualize_dataset?path=%2Fyuk6ra%2Fso101-tapes-cleanup%2Fepisode_0


# ファインチューニング編
- ARM: SO-ARM101
- DISK: 300GB
- RAM: 128GB
- CPU: 
- GPU: H100 SXM
- VRAM: 80GB
- Ubuntu 24.04
- Internet Upload Speed: 4Gbps
- Internet Download Speed: 4Gbps

ポイント、DISKは200GBぐらいはほしい。
Checkpointにもよる。


## 環境確認

リモートのsshで接続する

```sh
ssh -p 30454 root@xxx.xxx.xxx.xx -L 8080:localhost:8080
```

#### RAM
```sh
$ free -h
               total        used        free      shared  buff/cache   available
Mem:           503Gi        34Gi       372Gi        47Mi       101Gi       469Gi
Swap:          8.0Gi       186Mi       7.8Gi
```

### GPU
```sh
$ nvidia-smi
Sun Jul 13 06:57:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   47C    P0             73W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
```

### OS
```sh
$ lsb_release -d
No LSB modules are available.
Description:    Ubuntu 24.04.2 LTS
```

### Disk
```sh
$ df /home -h
Filesystem      Size  Used Avail Use% Mounted on
overlay         300G   90M  300G   1% /
```




## 環境構築

https://huggingface.co/spaces/lerobot/visualize_dataset


## 公式に習って


```sh
git clone https://github.com/NVIDIA/Isaac-GR00T
cd Isaac-GR00T
conda create -n gr00t python=3.10
conda activate gr00t
pip install --upgrade setuptools
pip install -e .[base]
pip install --no-build-isolation flash-attn==2.7.1.post4
```


## ログイン

```sh
huggingface-cli login
```

アクセスキーを取る

```sh
wandb login
```

https://wandb.ai/


## 学習データのダウンロード

```sh
huggingface-cli download \
       --repo-type dataset yuk6ra/so101-onetape-cleanup \
       --local-dir ./demo_data/so101-onetape-cleanup
```


```sh
cp getting_started/examples/so100_dualcam__modality.json ./demo_data/so101-onetape-cleanup/meta/modality.json
```

今回は
```sh
vim ./demo_data/so101-onetape-cleanup/meta/modality.json
```

```json
 {
        "state": {
            "single_arm": {
                "start": 0,
                "end": 5
            },
            "gripper": {
                "start": 5,
                "end": 6
            }
        },
        "action": {
            "single_arm": {
                "start": 0,
                "end": 5
            },
            "gripper": {
                "start": 5,
                "end": 6
            }
        },
        "video": {
            "front": {
                "original_key": "observation.images.front"
            },
            "tip": {　// 変更
                "original_key": "observation.images.tip"  // 変更
            }
        },
        "annotation": {
            "human.task_description": {
                "original_key": "task_index"
            }
        }
    }
```

```
python scripts/load_dataset.py --dataset-path ./demo_data/so101-onetape-cleanup --plot-state-action --video-backend torchvision_av
```

```sh
====================================================================================================
========================================= Humanoid Dataset =========================================
====================================================================================================
{'action.gripper': 'np scalar: 1.1111111640930176 [1, 1] float64',
 'action.single_arm': 'np: [1, 5] float64',
 'annotation.human.task_description': ['Grab the tape and place it in the '
                                       'box.'],
 'state.gripper': 'np scalar: 2.410423517227173 [1, 1] float64',
 'state.single_arm': 'np: [1, 5] float64',
 'video.front': 'np: [1, 480, 640, 3] uint8',
 'video.tip': 'np: [1, 480, 640, 3] uint8'}
dict_keys(['video.front', 'video.tip', 'state.single_arm', 'state.gripper', 'action.single_arm', 'action.gripper', 'annotation.human.task_description'])
==================================================
video.front: (1, 480, 640, 3)
video.tip: (1, 480, 640, 3)
state.single_arm: (1, 5)
state.gripper: (1, 1)
action.single_arm: (1, 5)
action.gripper: (1, 1)
annotation.human.task_description: ['Grab the tape and place it in the box.']
Image 0, prompt: ['Grab the tape and place it in the box.']
Image 10, prompt: ['Grab the tape and place it in the box.']
Image 20, prompt: ['Grab the tape and place it in the box.']
Image 30, prompt: ['Grab the tape and place it in the box.']
Image 40, prompt: ['Grab the tape and place it in the box.']
Image 50, prompt: ['Grab the tape and place it in the box.']
Image 60, prompt: ['Grab the tape and place it in the box.']
Image 70, prompt: ['Grab the tape and place it in the box.']
Image 80, prompt: ['Grab the tape and place it in the box.']
Image 90, prompt: ['Grab the tape and place it in the box.']
Image 100, prompt: ['Grab the tape and place it in the box.']
Image 110, prompt: ['Grab the tape and place it in the box.']
Image 120, prompt: ['Grab the tape and place it in the box.']
Image 130, prompt: ['Grab the tape and place it in the box.']
Image 140, prompt: ['Grab the tape and place it in the box.']
Image 150, prompt: ['Grab the tape and place it in the box.']
Image 160, prompt: ['Grab the tape and place it in the box.']
Image 170, prompt: ['Grab the tape and place it in the box.']
Image 180, prompt: ['Grab the tape and place it in the box.']
Image 190, prompt: ['Grab the tape and place it in the box.']
Warning: Skipping left_arm as it's not found in both state and action dictionaries
Warning: Skipping right_arm as it's not found in both state and action dictionaries
Warning: Skipping left_hand as it's not found in both state and action dictionaries
Warning: Skipping right_hand as it's not found in both state and action dictionaries
Plotted state and action space
```

```sh
vim ./gr00t/experiment/data_config.py
```

225行目あたり。
```python
class So100DualCamDataConfig(So100DataConfig):
     - video_keys = ["video.front", "video.wrist"]
     + video_keys = ["video.front", "video.tip"]
      	state_keys = ["state.single_arm", "state.gripper"]
       action_keys = ["action.single_arm", "action.gripper"]
       language_keys = ["annotation.human.task_description"]
       observation_indices = [0]
       action_indices = list(range(16))
```


## 学習させる

H100で30分ほど放置。
MAX5000ステップで100GBほどを消費。

```sh
python scripts/gr00t_finetune.py \
      --dataset-path ./demo_data/so101-onetape-cleanup/ \
      --num-gpus 1 \
      --output-dir ./so101-checkpoints  \
      --max-steps 5000 \
      --data-config so100_dualcam \
      --video-backend torchvision_av
```


### エラー①
もし下記のようなエラーがでるなら。
```sh
  0%|                                                                                                                            | 0/5000 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/workspace/Isaac-GR00T/scripts/gr00t_finetune.py", line 315, in <module>
    main(config)
  File "/workspace/Isaac-GR00T/scripts/gr00t_finetune.py", line 287, in main
    experiment.train()
  File "/workspace/Isaac-GR00T/gr00t/experiment/runner.py", line 173, in train
    self.trainer.train(resume_from_checkpoint=self.resume_from_checkpoint)
  File "/workspace/Isaac-GR00T/gr00t/experiment/trainer.py", line 153, in train
    return super().train(resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
  File "/venv/gr00t/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/venv/gr00t/lib/python3.10/site-packages/transformers/trainer.py", line 2514, in _inner_training_loop
    batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches, args.device)
  File "/venv/gr00t/lib/python3.10/site-packages/transformers/trainer.py", line 5243, in get_batch_samples
    batch_samples.append(next(epoch_iterator))
  File "/venv/gr00t/lib/python3.10/site-packages/accelerate/data_loader.py", line 552, in __iter__
    current_batch = next(dataloader_iter)
  File "/venv/gr00t/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
    data = self._next_data()
  File "/venv/gr00t/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1465, in _next_data
    return self._process_data(data)
  File "/venv/gr00t/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1491, in _process_data
    data.reraise()
  File "/venv/gr00t/lib/python3.10/site-packages/torch/_utils.py", line 714, in reraise
    raise RuntimeError(msg) from None
RuntimeError: Caught MemoryError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/venv/gr00t/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 351, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/venv/gr00t/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/venv/gr00t/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/workspace/Isaac-GR00T/gr00t/data/dataset.py", line 508, in __getitem__
    return self.transforms(self.get_step_data(trajectory_id, base_index))
  File "/workspace/Isaac-GR00T/gr00t/data/dataset.py", line 542, in get_step_data
    data[key] = self.get_data_by_modality(trajectory_id, modality, key, base_index)
  File "/workspace/Isaac-GR00T/gr00t/data/dataset.py", line 802, in get_data_by_modality
    return self.get_video(trajectory_id, key, base_index)
  File "/workspace/Isaac-GR00T/gr00t/data/dataset.py", line 672, in get_video
    return get_frames_by_timestamps(
  File "/workspace/Isaac-GR00T/gr00t/utils/video.py", line 112, in get_frames_by_timestamps
    for frame in reader:
  File "/venv/gr00t/lib/python3.10/site-packages/torchvision/io/video_reader.py", line 200, in __next__
    frame = next(self._c)
  File "av/container/input.pyx", line 208, in decode
  File "av/packet.pyx", line 80, in av.packet.Packet.decode
  File "av/video/stream.pyx", line 41, in av.video.stream.VideoStream.decode
  File "av/video/stream.pyx", line 50, in av.video.stream.VideoStream.decode
  File "av/codec/context.pyx", line 462, in av.codec.context.CodecContext.decode
  File "av/codec/context.pyx", line 238, in av.codec.context.CodecContext.open
  File "av/error.pyx", line 326, in av.error.err_check
av.error.MemoryError: [Errno 12] Cannot allocate memory
```

いったんPyAVをアップデートする
```sh
pip install -U av
```

### エラー②



## HFへモデルをアップロードする
```sh
cd so101-checkpoints/checkpoint-5000/
huggingface-cli upload yuk6ra/so101-onetape-cleanup . --commit-message="2000 step"
```

# 推論編（ローカル）
推論サーバーとクライアントサーバーに分かれています。

## 環境構築

```sh
git clone https://github.com/NVIDIA/Isaac-GR00T
cd Isaac-GR00T
conda create -n gr00t python=3.10
conda activate gr00t
pip install --upgrade setuptools
pip install -e .[base]
pip install --no-build-isolation flash-attn==2.7.1.post4
```

## モデルのダウンロード

推論サーバーが別PCであれば、もう一度GR00T N1の環境をつくる。

同様の環境を作る。



```sh
huggingface-cli login
huggingface-cli download yuk6ra/so101-onetape-cleanup --repo-type model --local-dir ./model/so101-onetape-cleanup
```

### 

code:gr00t/experiment/data_config.pyには注意する。

225行目

```python
class So100DualCamDataConfig(So100DataConfig):
- video_keys = ["video.front", "video.wrist"]
+ video_keys = ["video.front", "video.tip"]
    state_keys = ["state.single_arm", "state.gripper"]
    action_keys = ["action.single_arm", "action.gripper"]
    language_keys = ["annotation.human.task_description"]
    observation_indices = [0]
    action_indices = list(range(16))
```


## 評価する

```sh
huggingface-cli download \
       --repo-type dataset yuk6ra/so101-onetape-cleanup \
       --local-dir ./demo_data/so101-onetape-cleanup
cp getting_started/examples/so100_dualcam__modality.json ./demo_data/so101-onetape-cleanup/meta/modality.json
vim ./demo_data/so101-onetape-cleanup/meta/modality.json
```
治す。

```sh
python scripts/eval_policy.py --plot \
       --embodiment_tag new_embodiment \
       --model_path ./model/so101-onetape-cleanup/ \
       --data_config so100_dualcam \
       --dataset_path ./demo_data/so101-onetape-cleanup/ \
       --video_backend torchvision_av \
       --modality_keys single_arm gripper \
       --denoising_steps 4
```

## 推論サーバーの立ち上げ
### ローカル環境
- 4070 Ti 12GB

```sh
$ nvidia-smi
Sun Jul 13 19:45:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.153.02             Driver Version: 570.153.02     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4070 Ti     Off |   00000000:01:00.0  On |                  N/A |
|  0%   34C    P8              6W /  285W |     682MiB /  12282MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            2453      G   /usr/lib/xorg/Xorg                      309MiB |
|    0   N/A  N/A            2608      G   /usr/bin/gnome-shell                     70MiB |
|    0   N/A  N/A            3211      G   /opt/brave.com/brave/brave                3MiB |
|    0   N/A  N/A            3254      G   ...c0b8c13d56e6779503f2cc987a424        119MiB |
|    0   N/A  N/A            5526      G   ...ess --variations-seed-version        125MiB |
+-----------------------------------------------------------------------------------------+
```


```sh
$ lsb_release -d
Description:	Ubuntu 22.04.5 LTS
```

立ち上げる
```sh
python scripts/inference_service.py --model_path ./model/so101-onetape-cleanup --embodiment_tag new_embodiment --data_config so100_dualcam --server --port 5555
```


####　エラー
```sh
pip install --no-build-isolation flash-attn==2.7.1.post4 

Collecting flash-attn==2.7.1.post4
  Using cached flash_attn-2.7.1.post4.tar.gz (2.7 MB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [20 lines of output]
      fatal: not a git repository (or any of the parent directories): .git
      /tmp/pip-install-myxzi7su/flash-attn_3ea6f071d2e84485a9e98af6137eb7b7/setup.py:99: UserWarning: flash_attn was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.
        warnings.warn(
      Traceback (most recent call last):
        File "<string>", line 2, in <module>
        File "<pip-setuptools-caller>", line 35, in <module>
        File "/tmp/pip-install-myxzi7su/flash-attn_3ea6f071d2e84485a9e98af6137eb7b7/setup.py", line 184, in <module>
          CUDAExtension(
        File "/home/yuk6ra/anaconda3/envs/gr00t/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 1078, in CUDAExtension
          library_dirs += library_paths(cuda=True)
        File "/home/yuk6ra/anaconda3/envs/gr00t/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 1209, in library_paths
          if (not os.path.exists(_join_cuda_home(lib_dir)) and
        File "/home/yuk6ra/anaconda3/envs/gr00t/lib/python3.10/site-packages/torch/utils/cpp_extension.py", line 2416, in _join_cuda_home
          raise OSError('CUDA_HOME environment variable is not set. '
      OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.
      
      
      torch.__version__  = 2.5.1+cu124
      
      
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```

これを入れる
```
conda install -c nvidia cuda-toolkit=12.4
```

### クラウド環境

### エラー
もし`ModuleNotFoundError: No module named 'flash_attn'`が出たら、仮想環境が`gr00t`ではなく`lerobot`や`base`の環境になっていないかを確認する。

## クライアントサーバーの立ち上げ

注意：LeRobotのライブラリと互換性を保つ必要があります。



lerobotの環境をつくるやる。もうすでにあればOKで学習データを収集したときのものでOK。

```sh
git clone https://github.com/huggingface/lerobot.git
cd lerobot
conda create -y -n lerobot python=3.10
conda activate lerobot
conda install ffmpeg -c conda-forge
pip install -e .
pip install -e ".[feetech]"
```

参考文献：https://huggingface.co/docs/lerobot/installation


このときlerobot のフォルダだと思いますが、
Isaac-GR00Tのフォルダに移動して環境を切り替える。

```sh
d ~/Documents/Isaac-GR00T/
conda activate lerobot
```

さらに追加でパッケージとgr00tのパッケージのダウンロード。
```sh
pip install matplotlib

pip install --upgrade setuptools
pip install -e .[base]
pip install --no-build-isolation flash-attn==2.7.1.post4
```


`outputs/captured_images`以下でカメラのIDをメモしておく。
```sh
python -m lerobot.find_cameras opencv
```

`getting_started/examples/eval_lerobot.py`で、lerobotのバージョンがあっていないので`common`を消す。

```python
from lerobot.cameras.opencv.configuration_opencv import ( # <---commonを消す
    OpenCVCameraConfig,
)
from lerobot.robots import ( # <---commonを消す
    Robot,model/so101-tapes-cleanup-2000
    RobotConfig,
    koch_follower,
    make_robot_from_config,
    so100_follower,
    so101_follower,
)
from lerobot.utils.utils import ( # <---commonを消す
    init_logging,
    log_say,
)

...
# from service import ExternalRobotInferenceClient # <--- コメントアウト

from gr00t.eval.service import ExternalRobotInferenceClient # <--- 
```

必要なパッケージをインストールする。
```
pip install matplotlib
```


```sh
python getting_started/examples/eval_lerobot.py \
         --robot.type=so101_follower \
         --robot.port=/dev/ttyACM1 \
         --robot.id=white \
         --robot.cameras="{
             tip: {type: opencv, index_or_path: 2, width: 640, height: 480, fps: 30},
             front: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}
         }" \
         --lang_instruction="Grab the tape and place it in the box."
```

### クラウド向け

イメージなどに下記のポートをあけてください。
```
-p 5555:5555
```

```sh
python getting_started/examples/eval_lerobot.py \
       --robot.type=so101_follower \
       --robot.port=/dev/ttyACM0 \
       --robot.id=white \
       --robot.cameras="{
           tip: {type: opencv, index_or_path: 2, width: 640, height: 480, fps: 30},
           front: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}
       }" \
       --policy_host 194.14.47.19 \ # 追加
       --policy_port 22037 \ # 追加
       --lang_instruction="Grab tapes and place into pen holder."
```


# 評価編



# まとめ
